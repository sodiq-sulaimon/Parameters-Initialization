{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0882678a",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931266b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Weights initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa37a8",
   "metadata": {},
   "source": [
    "We'll use a 3-layer neural network. These are the initialization methods you'll experiment with: \n",
    "- *Zeros initialization* --  setting `initialization = \"zeros\"` in the input argument.\n",
    "- *Random initialization* -- setting `initialization = \"random\"` in the input argument. This initializes the weights to large random values.  \n",
    "- *He initialization* -- setting `initialization = \"he\"` in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9412ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583d8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):     \n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the loss\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60cfa2",
   "metadata": {},
   "source": [
    "#### 1. Zero Initialization\n",
    "There are two types of parameters to initialize in a neural network:\n",
    "- the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdf75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb1946d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[0. 0.]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "parameters = initialize_parameters_zeros([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb01277",
   "metadata": {},
   "source": [
    "#### 2. Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1edb685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # integer representing the number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835db235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[  8.81318042  17.09573064   0.50033642]\n",
      " [ -4.04677415  -5.45359948 -15.46477316]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[  9.82367434 -11.0106763 ]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835b2fe",
   "metadata": {},
   "source": [
    "#### 3. He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b37301e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        #(â‰ˆ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = \n",
    "        # parameters['b' + str(l)] =\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "                \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c09af7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.78862847  0.43650985]\n",
      " [ 0.09649747 -1.8634927 ]\n",
      " [-0.2773882  -0.35475898]\n",
      " [-0.08274148 -0.62700068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42f39a",
   "metadata": {},
   "source": [
    "#### Things to remember\n",
    "- Different initializations lead to very different results\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "- Resist initializing to values that are too large!\n",
    "- He initialization works well for networks with ReLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2855a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
